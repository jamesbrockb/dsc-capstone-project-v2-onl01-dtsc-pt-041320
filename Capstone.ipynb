{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was created to help us understand what it takes, lyrically, to be in the Billboard Top 100 Charts. The data explored is largely from the years between 1999-2019, however there is some additional data that dates back to 1959. \n",
    "\n",
    "Throughout the notebook, we will be extensively cleaning our data and performing three separate models. The models being ran throughout the notebook are Random Forests, XGB and SVM. Additionally there will be further data exploration on how genre can play an intricate role when it comes to being involved in the Billboard's top 100 charts,. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variable was created as to not run cell 7. Cell 7 is where I scraped for both a genius API and a spotify API to get lyrics as well as certain spotify features. If you'd like to run cell 7 to see how it works, please feel free to change False to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = False\n",
    "api_file = '/Users/jamesbrochhausen/.secret/Spotify.json'\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score\n",
    "from xgboost import XGBRFClassifier,XGBClassifier\n",
    "import shap\n",
    "from imblearn.over_sampling import SMOTE\n",
    "shap.initjs()\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEED TO FIND SPOTIFY 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Used for songs not in top 100. FIND WHERE SPOTIFY 2000.CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rando = pd.read_csv('Spotify-2000.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_rando.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spotify and Genius API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install lyricsgenius\n",
    "# pip install spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scrape:\n",
    "    import json\n",
    "\n",
    "    with open(api_file) as file:\n",
    "        login = json.loads(file.read())\n",
    "\n",
    "    login.keys()\n",
    "\n",
    "    import spotipy\n",
    "    from spotipy.oauth2 import SpotifyClientCredentials\n",
    "\n",
    "    sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=login['client_id'],\n",
    "                                                               client_secret=login['client_secret']))\n",
    "\n",
    "    df_rando['Lyrics']=''\n",
    "    df_rando.head()\n",
    "\n",
    "    import lyricsgenius\n",
    "    genius = lyricsgenius.Genius(login['genius_secret'])\n",
    "\n",
    "    def get_lyrics(x):\n",
    "        title = x['Title']\n",
    "        artist = x['Artist']\n",
    "\n",
    "        song = genius.search_song(title, artist)\n",
    "    #     print(song.lyrics)\n",
    "        try: \n",
    "            return song.lyrics\n",
    "        except:\n",
    "            return 'Not Found'\n",
    "    print('Scraping data')\n",
    "    df_rando['Lyrics']=df_rando.apply(lambda x: get_lyrics(x), axis=1)\n",
    "\n",
    "    df_rando.to_csv('randomsongs.csv',index=False)\n",
    "else:\n",
    "     print ('Skipping since scrape == False')   \n",
    "# Change this to a relative file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rando = pd.read_csv('randomsongs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_rando.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_rando['Lyrics'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've confirmed all the lyrics are in the songs, I need to go through the Lyrics column and removes words like Chorus and Bridge and Verse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing words from rows in Lyrics Column\n",
    "\n",
    "word_list = ['[Chorus 1]','[Chorus 2]','[Chorus 3]','[Chorus 4]',\n",
    "             '[Verse 1]','[Verse 2]','[Verse 3]','[Verse 4]','[Bridge]',\n",
    "             '[Intro]','[Chorus]','[Outro]','[outro]','[Verse]','[verse]',\n",
    "            '[Pre-Chorus]','[pre-chorus]','[Instrumental]','[instrumental]',\n",
    "             '[post-chorus]','[Post-Chorus]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in word_list:\n",
    "#     for lyric in df_rando['Lyrics']:\n",
    "# #     if word in df_rando['Lyrics']:\n",
    "#         if word in lyric:\n",
    "# #             print(word)\n",
    "#             df_rando['Lyrics'].replace((word,''),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(word_list, lyrics):\n",
    "#     print(lyrics)\n",
    "    for word in word_list:\n",
    "        if word in lyrics:\n",
    "            lyrics = lyrics.replace(word,'')\n",
    "#             print(word)\n",
    "    return lyrics\n",
    "\n",
    "\n",
    "df_rando['Lyrics']=df_rando['Lyrics'].apply(lambda x: replace_words(word_list,\n",
    "                                                                    x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rando['Lyrics'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Billboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add your markdowns\n",
    "df_bb = pd.read_csv('billboardHot100_1999-2019.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecassary columns for our solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_bb = df_bb.drop(columns=['Features','Writing.Credits','Week','Date'])\n",
    "df_bb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_bb['Lyrics'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for duplicates\n",
    "df_bb['Name'].duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm going to eventually be joining this column with my Hot Stuff column below, I'm going to need to remove and and all duplicates before I can initiate that process down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_bb['Name'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset above houses duplicate songs as they appear multiple times in a week and multiple times throughout a year. Which makes sense why there are so many. What we care about most however is the lyrics and how similar they are to one another. So we're going to drop all duplicate lyrics (just in case there are some songs with the same title)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb['Lyrics'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb[df_bb['Lyrics'].duplicated(keep=False)].sort_values('Lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb = df_bb.drop_duplicates(subset=['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_bb.shape)\n",
    "df_bb.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying all the lyrics are in the actual song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_bb['Lyrics'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hot Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May use this as well with an API call to create a data frame that includes more of the other information. But for now we're focused on achieving lyrical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot_audio_features = pd.read_csv('Hot 100 Audio Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot_audio_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Further removing features I don't want included\n",
    "\n",
    "df_hot_audio_features = df_hot_audio_features.drop(columns=['spotify_track_id',\n",
    "                                    'spotify_track_preview_url',\n",
    "                     'spotify_track_album','spotify_track_explicit',\n",
    "                     'spotify_track_duration_ms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking duplicates again to remove before joining our two data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_hot_audio_features['SongID'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot_audio_features['Song'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot_audio_features = df_hot_audio_features.drop_duplicates(subset=['Song'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot_audio_features['Song'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot_audio_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_hot_audio_features.shape)\n",
    "df_bb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hot_audio_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hot_stuff.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data Frames on Song Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Columns on both Artists and Song name\n",
    "\n",
    "print(df_bb.shape, df_hot_audio_features.shape)\n",
    "master_df = pd.merge(df_hot_audio_features, df_bb,\n",
    "                  left_on=['Performer','Song'], right_on=['Artists','Name'],\n",
    "                     how='inner')\n",
    "\n",
    "\n",
    "\n",
    "# print(df_bb.shape)\n",
    "# print(df_hot_audio_features.shape)\n",
    "# print()\n",
    "print(master_df.shape)\n",
    "master_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with duplicates round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## One duplicate\n",
    "master_df['Song'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove last duplicate\n",
    "master_df = master_df.drop_duplicates(subset=['Song'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df['Song'].duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "master_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rando.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns in df_rando that are not in master df and vice versa aside from the billboard rankings which we will fill out later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title, yes\n",
    "# artist, yes\n",
    "# year, no\n",
    "# bpm, no\n",
    "# energy, yes\n",
    "# dance, yes\n",
    "# loudness, yes\n",
    "# liveness, yes\n",
    "# valence, yes\n",
    "# length, yes\n",
    "# accousticness, yes\n",
    "# speechiness, yes\n",
    "# pupularity, yes\n",
    "\n",
    "\n",
    "\n",
    "df_rando = df_rando.drop(['Beats Per Minute (BPM)','Year'], axis=1)\n",
    "df_rando.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tempo, no\n",
    "# instrumental, no\n",
    "# mode, no\n",
    "# key, no\n",
    "# songid, no\n",
    "\n",
    "master_df = master_df.drop(['SongID','tempo','instrumentalness','mode','key','Artists','Name'],axis=1)\n",
    "master_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Null values in our features with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = master_df.fillna(master_df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill no genres with 'no genre'\n",
    "master_df[\"spotify_genre\"].fillna(\"['No Genre']\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['Weekly.rank'] = master_df['Weekly.rank'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rando.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rando.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'm going to rename the columns in my master data frame to map our random data frame. This way when we join we will eliminate duplicate columns appearing. It will also look much cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.rename(columns={'Song':'Title', 'Performer':'Artist',\n",
    "                         'spotify_genre':'Top Genre',\n",
    "                          'spotify_track_popularity':'Popularity',\n",
    "                         'danceability':'Danceability','loudness':'Loudness (dB)',\n",
    "                        'energy':'Energy','speechiness':'Speechiness',\n",
    "                        'acousticness':'Acousticness','liveness':'Liveness',\n",
    "                          'valence':'Valence','time_signature':'Length (Duration)',\n",
    "                        }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "master_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.loc[0,'Top Genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now our top genre columns contents are a string. Python’s eval() allows me to evaluate  expressions from a string-based or compiled-code-based input. Which will basically help me convert this back into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experimenting, it worked\n",
    "eval(master_df.loc[0,'Top Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## applying to the entire column\n",
    "master_df['Top Genre'] = master_df['Top Genre'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(master_df.loc[0,'Top Genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now my goal is to turn this back into a string, by leveraging our .join function. I'm doing this so that when I join our master_df and rando_df below they'll be in the same 'format'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(master_df.loc[0,'Top Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['Top Genre'] = [', '.join(map(str, l)) for l in master_df['Top Genre']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = pd.concat([master_df,df_rando])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_df_2 = master_df.explode('Top Genre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df_2 = pd.concat([master_df_2,df_rando])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df_2['Top Genre'].value_counts().head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final_df_2['Top Genre'].value_counts().head(20).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rep = {'pop':'Pop', 'contemporary country':'Country', 'country':'Country',\n",
    "#        'country road':'Country'}\n",
    "#        'pop rap', 'post-teen pop', 'rap', 'hip hop', 'r&b',\n",
    "#        'modern country rock', 'album rock', 'urban contemporary', 'pop rock',\n",
    "#        'trap', 'hip pop', 'post-grunge', 'neo mellow', 'southern hip hop',\n",
    "#        'alternative metal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df_2['Top Genre'].map(rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df['df'] = 'master_df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rando['df'] = 'rando_df'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the my column names are the same (except for the ones with additional data) we can now join the two data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_master = pd.concat([master_df,df_rando])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rando.shape)\n",
    "print(master_df.shape)\n",
    "potential_master.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = potential_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.shape)\n",
    "final_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Genre and my Top Genre has the all the genres in it now. And we can see that all of the genres mapped over correctly into Top Genre as there are no null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.drop(['Genre'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Text / EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = final_df['Lyrics'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ls = stopwords.words('english')\n",
    "stopwords_ls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords_ls.extend(string.punctuation)\n",
    "stopwords_ls[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the normal, word_tokenize function doesn't know how to effectively read words from my experience. For example the word can't would be produced as \"can\" \"'t\". Which is not what we want, we want the full words. I found that the tweet tokenizer is able to handle this dataset more appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tknzr.tokenize('.'.join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_tokens = [w.lower() for w in token if w.lower() not in stopwords_ls]\n",
    "removed_tokens[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq = FreqDist(removed_tokens)\n",
    "freq.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ls.extend(['—','2018','scp','’','”','“','feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_tokens = [w.lower() for w in token if w.lower() not in stopwords_ls]\n",
    "removed_tokens[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(removed_tokens)\n",
    "freq.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=stopwords_ls, collocations=False)\n",
    "wordcloud.generate(','.join(removed_tokens))\n",
    "\n",
    "plt.figure(figsize = (15,15), facecolor = 'black')\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "lyric = nltk.BigramCollocationFinder.from_words(removed_tokens)\n",
    "lyric_freq = lyric.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lyric_freq, columns=[\"Lyric\",\"Freq\"]).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df['Top Genre']\n",
    "# .value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_corpus = final_df['Top Genre'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing the most popular genres and recognizing there are similarites\n",
    "# FreqDist(genre_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['Genre'] = final_df['Genre'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Took most popular genres\n",
    "# ids = {'id':['Country','Metal','Pop','Classical','Rock','Electronic',\n",
    "#              'Rap','R&;B'],\n",
    "#       'Genre':['Country','Metal','Pop','Classical','Rock','Electronic',\n",
    "#                      'Rap','R&;B']}\n",
    "# ids = dict(zip(ids['id'], ids['Genre']))               \n",
    "# print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['Top Genre'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['Top Genre'] = [str(i) for i in final_df['Top Genre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final_df['Top Genre'] = final_df['Top Genre'].apply(lambda x: x.upper()) \n",
    "\n",
    "# final_df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Viewing the most popular genres and recognizing there are similarites\n",
    "FreqDist(genre_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm importing re as this method takes a regular expression pattern and a string and searches for that pattern within the string. This helps set me up for when I need to make our genres just one singular generic genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "## Most frequent / common genres\n",
    "rep = ['Country','Metal','Pop','Classical','Rock','Electronic',\n",
    "       'Rap','R&;B','Adult Standards','Indie','Cabaret','Hip Hop','Soul']\n",
    "final_df['Top Genre_'] = [re.findall(r'|'.join(rep), i,\n",
    "                                 re.IGNORECASE) for i in final_df['Top Genre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df['Top Genre_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going through my Top Genre_ list and making sure that the genres don't repeat in their cell like we saw above. eq() will perform comparisons of my dataframe column objects with constant, series or another dataframes objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Top Genre'][final_df['Top Genre_'].str.len().eq(0)].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that this is somewhat cleaned up. I need to replace every genre that's not in 'rep' with 'unique genre'. This will make the information more digestible, rather than having a genre look like 'cyberpunk'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_genre(rep, genre):\n",
    "    #if genre has nothing in it or whitespace\n",
    "    if len(genre)==0:\n",
    "        return \"Unique Genre\"\n",
    "    else:\n",
    "        return max(set(genre), key=genre.count)\n",
    "final_df['Top Genre_']=final_df['Top Genre_'].apply(lambda x: replace_genre(rep,\n",
    "                                                                            x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Top Genre_'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['Top Genre'][final_df['Top Genre_'].str.len().eq(0)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# rep = ['Country','Metal','Pop','Classical','Rock','Electronic',\n",
    "#              'Rap','R&;B'] ## add other genres here\n",
    "# final_df['Top Genre_'] = [re.findall(r'|'.join(rep), i,\n",
    "#                                  re.IGNORECASE)[0] for i in final_df['Top Genre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(final_df['Top Genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Top Genre_'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filling null with 0's\n",
    "\n",
    "final_df['Weekly.rank'].fillna(value=0, inplace=True)\n",
    "final_df['Peak.position'].fillna(value=0, inplace=True)\n",
    "final_df['Weeks.on.chart'].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy import median\n",
    "\n",
    "sns.catplot(x=\"Top Genre_\", y=\"Weekly.rank\", kind=\"bar\",\n",
    "            estimator=np.median,data=final_df,\n",
    "            height=10,aspect=11/10)\n",
    "sns.swarmplot(x=\"Top Genre_\", y=\"Weekly.rank\", data=final_df,edgecolor='black',\n",
    "              linewidth=0.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Top Genre_\", y=\"Peak.position\", kind=\"bar\",\n",
    "            estimator=np.median,data=final_df,\n",
    "            height=10,aspect=11/10)\n",
    "sns.swarmplot(x=\"Top Genre_\", y=\"Peak.position\", data=final_df)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x=\"Top Genre_\", y=\"Weeks.on.chart\", kind=\"bar\",\n",
    "            estimator=np.median,data=final_df,\n",
    "            height=10,aspect=11/10)\n",
    "sns.swarmplot(x=\"Top Genre_\", y=\"Weeks.on.chart\", data=final_df)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPELL  OUT WHY ON THE BELOW. Create a top 100 column which shows us if this song was in the billboard top 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "final_df['top_100'] = (final_df['Peak.position']>=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.sort_values(['Artist','Title','df'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df.duplicated(keep=False,\n",
    "                             subset=('Artist','Title'))]\n",
    "\n",
    "## look in to dropping duplicates here\n",
    "# .sort_values('Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb[df_bb['Lyrics'].duplicated(keep=False)].sort_values('Lyrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring how many made it\n",
    "final_df['top_100'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "final_df['top.100'] = le.fit_transform(final_df['top_100'])\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = final_df['top.100']\n",
    "X = final_df['Lyrics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leveraging tweet tokenizer again so it can capture the full word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.TweetTokenizer(preserve_case=False)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "## Initialize TfIdf Vectorizer, feed in function of tokenize\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize,\n",
    "                            stop_words=stopwords_ls)\n",
    "\n",
    "# Vectorize data and make X_train_tfidf and X_test_tfidf\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import RandomOverSampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(class_weight=\"balanced\")\n",
    "rf.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = rf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_arr = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "#     plt.grid(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate and view my model.\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "def evaluate_model(y_test,y_hat_test,X_test,clf=None,\n",
    "                  scoring=None,\n",
    "                   verbose=False,scorer=False,\n",
    "                   classes=['Not Top 100','Top 100'],\n",
    "                  normalize = 'true'):\n",
    "    \n",
    "    print(metrics.classification_report(y_test,y_hat_test,\n",
    "                                        target_names=classes))\n",
    "    if scoring is None:\n",
    "        scoring = metrics.recall_score(y_test,y_hat_test,average='macro')\n",
    "    \n",
    "    cm = metrics.confusion_matrix(y_test, y_hat_test,\n",
    "    normalize = normalize)\n",
    "    plot_confusion_matrix(cm,\n",
    "                      normalize    = False,\n",
    "                      target_names = classes,\n",
    "                      title        = \"Confusion Matrix\")\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"MODEL PARAMETERS:\")\n",
    "\n",
    "        print(pd.Series(rf.get_params()))\n",
    "## Use scoring = recall_macro in gridsearch.        \n",
    "    if scorer:\n",
    "        \n",
    "        return recall_macro(y_test,y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(y_test_arr,y_hat_test,X_test_tfidf,rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rf.feature_importances_,\n",
    "          index=vectorizer.get_feature_names()).sort_values().tail(25).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_rf = XGBClassifier()\n",
    "xgb_rf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = xgb_rf.predict(X_test_tfidf)\n",
    "\n",
    "evaluate_model(y_test_arr,y_pred2,X_test,xgb_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(xgb_rf.feature_importances_,\n",
    "          index=vectorizer.get_feature_names()).sort_values().tail(25).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train_tfidf,y_train)\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "evaluate_model(y_test_arr,y_pred,X_test_tfidf,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.292px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
